logi<-NULL
if(length(c(...))==0){
return()
}else{
data<-list(...)
for(item in data){
if(is.numeric(item)){
num <- append(num,item)
}else if(is.character(item)){
char <- append(char,item)
}else if(is.logical(item))
logi <- append(logi,item)
}
}
return(list(numb=num,char=char,logic=logi))
}
createVector()
createVector(10,2,"dd",exam1(10,20))
createVector(10,2,"dd",exam1(10,20),TRUE)
#실습7
createVector<-function(...){
num<-NULL
char<-NULL
logi<-NULL
if(length(c(...))==0){
return()
}else{
data<-list(...)
for(item in data){
if(is.numeric(item)){
num <- append(num,item)
}else if(is.character(item)){
char <- append(char,item)
}else if(is.logical(item))
logi <- append(logi,item)
}
}
return(list(numb=num,char=char,logic=logi))
}
createVector()
createVector(10,2,"dd",exam1(10,20),TRUE)
createVector(10,2,"dd",10,20,TRUE)
exam3(5,"$")
exam3<-function(num,char="#"){
for(data in 1:num)
cat(char)
return()
}
exam3(5,"$")
#실습7
createVector<-function(...){
num<-NULL #혹은 num<-c() 형식으로 해도 됌
char<-NULL
logi<-NULL
if(length(c(...))==0){
return()
}else{
data<-list(...)
for(item in data){
if(is.numeric(item)){
num <- append(num,item)
}else if(is.character(item)){
char <- append(char,item)
}else if(is.logical(item))
logi <- append(logi,item)
}
}
return(list(number=num,character=char,logic=logi))
}
createVector()
createVector(10,2,"dd",10,20,TRUE)
cat("가장 많이 등장한 단어는",names(most),"입니다.")
#실습9
word<-scan("iotest2.txt",what = )
#실습9
word<-scan("iotest2.txt",what ="")
factor(word)
most<-sort(summary(factor(word)),decreasing = T)[1]
cat("가장 많이 등장한 단어는",names(most),"입니다.")
??prin
url<- "https://movie.daum.net/moviedb/grade?movieId=121137&type=netizen"
text <- read_html(url,  encoding="UTF-8")
text
library(rvest)
url<- "https://movie.daum.net/moviedb/grade?movieId=121137&type=netizen"
text <- read_html(url,  encoding="UTF-8")
text
# 영화평점
nodes <- html_nodes(text, ".emph_grade")
point <- html_text(nodes)
point
# 영화리뷰
nodes <- html_nodes(text, ".desc_review")
review <- html_text(nodes, trim=TRUE)
review
review <- gsub("\r", "", review)
review
page <- cbind(title, point)
page <- cbind(page, review)
page
page <- cbind(point, review)
page
write.csv(page, "daummovie1.csv")
library(rvest)
site<- "https://movie.daum.net/moviedb/grade?movieId=121137&type=netizen&page="
movie.review <- NULL
for (i in 1:20) {
url <- paste(site, i, sep="")
text <- read_html(url,  encoding="UTF-8")
# 영화평점
nodes <- html_nodes(text, ".emph_grade")
point <- html_text(nodes)
point
# 영화리뷰
nodes <- html_nodes(text, ".desc_review")
review <- html_text(nodes, trim=TRUE)
review
review <- gsub("\r", "", review)
review
page <- cbind(point, review)
movie.review <- rbind(movie.review, page)
}
write.csv(page, "daummovie2.csv")
movie.review
write.csv(movie.review, "daummovie2.csv")
write.csv(movie.review, "daummovie2.csv")
# 한국일보 페이지(XML 패키지 사용)
install.packages("XML")
library(XML)
t <- htmlParse("http://hankookilbo.com")
t <- htmlParse("http://hankookilbo.com")
t <- htmlParse("https://hankookilbo.com")
user_agent("chrome")
user_agent()
user_agent()
imsi <- htmlParse("http://hankookilbo.com")
t<- htmlParse(imsi)
library(XML)
imsi <- htmlParse("http://hankookilbo.com")
imsi <- html_html("http://hankookilbo.com")
imsi <- read_html("http://hankookilbo.com")
t<- htmlParse(imsi)
t
content<- xpathSApply(t,"//p[@class='title']", xmlValue);
content
content <- gsub("[[:punct:][:cntrl:]]", "", content)
content
content <- trimws(content)
content
# httr 패키지 사용 - GET 방식 요청
install.packages("httr")
install.packages("httr")
library(httr)
http.standard <- GET('http://www.w3.org/Protocols/rfc2616/rfc2616.html')
title2 = html_nodes(read_html(http.standard), 'div.toc h2')
title2 = html_text(title2)
title2
http.standard <- GET('http://www.w3.org/Protocols/rfc2616/rfc2616.html')
title2 = html_nodes(read_html(http.standard), 'div.toc h2')
# 단일 페이지(rvest 패키지 사용)
install.packages("rvest");
library(rvest)
title2 = html_nodes(read_html(http.standard), 'div.toc h2')
title2 = html_text(title2)
title2
http.standard <- GET('http://www.w3.org/Protocols/rfc2616/rfc2616.html')
title2 = html_nodes(read_html(http.standard), 'div.toc h2')
title2 = html_text(title2)
title2
# httr 패키지 사용 - POST 방식 요청
library(httr)
# POST 함수를 이용해 모바일 게임 랭킹 10월 29일 주  모바일 게임 랭킹을 찾는다
#(http://www.gevolution.co.kr/score/gamescore.asp?t=3&m=0&d=week)
game = POST('http://www.gevolution.co.kr/score/gamescore.asp?t=3&m=0&d=week',
encode = 'form', body=list(txtPeriodW = '2018-12-03'))
title2 = html_nodes(read_html(game), 'a.tracktitle')
title2 = html_text(title2)
title2[1:10]
# 뉴스, 게시판 등 글 목록에서 글의 URL만 뽑아내기
res = GET('https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&sid1=001')
htxt = read_html(res)
htxt
link = html_nodes(htxt, 'div.list_body a')
article.href = unique(html_attr(link, 'href'))
article.href
length(link)
article.href = unique(html_attr(link, 'href'))
article.href
link = html_nodes(htxt, 'div.list_body a'); length(link)
article.href = unique(html_attr(link, 'href'))
article.href
?writeBin
# 이미지, 첨부파일 다운 받기
# pdf
res = GET('http://cran.r-project.org/web/packages/httr/httr.pdf')
writeBin(content(res, 'raw'), 'c:/Temp/httr.pdf')
?writeBin
# jpg
h = read_html('http://unico2013.dothome.co.kr/productlog.html')
imgs = html_nodes(h, 'img')
img.src = html_attr(imgs, 'src')
for(i in 1:length(img.src)){
res = GET(paste('http://unico2013.dothome.co.kr/',img.src[i], sep=""))
writeBin(content(res, 'raw'), paste('c:/Temp/', img.src[i], sep=""))
}
library(rvest)
site<- "http://media.daum.net/ranking/popular/"
movie.review <- NULL
url<- "http://media.daum.net/ranking/popular/"
text <- read_html(url,  encoding="UTF-8")
# newtitle
nodes <- html_nodes(text, ".link_txt")
newstitle <- html_text(nodes)
newstitle
# newtitle
nodes <- html_nodes(text, "strong.link_txt")
newstitle <- html_text(nodes)
newstitle
# newtitle
nodes <- html_nodes(text, ".link_txt")
newstitle <- html_text(nodes)
newstitle
# newtitle
nodes <- html_nodes(text, "a.link_txt")
newstitle <- html_text(nodes)
newstitle
# newtitle
nodes <- html_nodes(text, "strong.tit_thumb a.link_txt")
newstitle <- html_text(nodes)
newstitle
text <- read_html(url,  encoding="UTF-8")
# newtitle
nodes <- html_nodes(text, "strong.tit_thumb a.link_txt")
newstitle <- html_text(nodes)
newstitle
# newtitle
nodes <- html_nodes(text, "div.cont_thumb strong.tit_thumb a.link_txt")
newstitle <- html_text(nodes)
newstitle
# newtitle
nodes <- html_nodes(text, "ul.list_news2 a.link_txt")
newstitle <- html_text(nodes)
newstitle
# newspaper
nodes <- html_nodes(text, "ul.list_news2 a.info_news")
newspapername <- html_text(nodes, trim=TRUE)
newspapername
# newspaper
nodes <- html_nodes(text, "ul.list_news2 span.link_txt")
newspapername <- html_text(nodes, trim=TRUE)
newspapername
newstitle <- gsub("[[:punct:][:cntrl:]]", "", newstitle)
newstitle
newspapername <- gsub("[[:punct:][:cntrl:]]", "", newspapername)
newspapername
page <- cbind(newstitle, newspapername)
# newspaper
nodes <- html_nodes(text, "ul.list_news2 span.info_news")
newspapername <- html_text(nodes, trim=TRUE)
newspapername <- gsub("[[:punct:][:cntrl:]]", "", newspapername)
newspapername
page <- cbind(newstitle, newspapername)
page
write.csv(page, "daumnews.csv")
library(rvest)
url<- "http://www.saramin.co.kr/zf_user/search?search_area=main&search_done=y&search_optional_item=n&searchType=default_mysearch&searchword=%EC%9E%90%EB%B0%94"
text <- read_html(url,  encoding="UTF-8")
# newtitle
nodes <- html_nodes(text, "li.swiper-slide add-filter swiper-slide-next span.txt")
newstitle <- html_text(nodes)
newstitle <- gsub("[[:punct:][:cntrl:]]", "", newstitle)
newstitle
# newtitle
nodes <- html_nodes(text, "li.swiper-slide add-filter swiper-slide-next")
newstitle <- html_text(nodes)
newstitle <- gsub("[[:punct:][:cntrl:]]", "", newstitle)
newstitle
# newtitle
nodes <- html_nodes(text, "span.txt")
newstitle <- html_text(nodes)
newstitle <- gsub("[[:punct:][:cntrl:]]", "", newstitle)
newstitle
# newtitle
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- gsub("신고", "", tech_name)
tech_name
tech_name <- gsub("다른 필터항목 보기", "", tech_name)
tech_name
tech_name <- gsub("다른 필터항목 보기", NULL, tech_name)
tech_name
# newtitle
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- gsub("다른 필터항목 보기", NULL, tech_name)
tech_name
# newtitle
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- gsub("다른 필터항목 보기", "", tech_name)
tech_name
# newtitle
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- rm("다른 필터항목 보기")
tech_name
# newtitle
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name
# 건수
nodes <- html_nodes(text, "span.count")
newspapername <- gsub("[[:punct:][:cntrl:]]", "", newspapername)
newspapername
# 건수
nodes <- html_nodes(text, "span.count")
newspapername <- html_text(nodes, trim=TRUE)
newspapername <- gsub("[[:punct:][:cntrl:]]", "", newspapername)
newspapername
# 건수
nodes <- html_nodes(text, "span.count")
newspapername <- html_text(nodes, trim=TRUE)
newspapername
# 건수
nodes <- html_nodes(text, "span.count")
newspapername <- html_text(nodes, trim=TRUE)
newspapername <- gsub("[[:punct:][:cntrl:]]", "", newspapername)
tech_name <- gsub("건", "", tech_name)
newspapername
nodes <- html_nodes(text, "span.count")
info_count <- html_text(nodes, trim=TRUE)
info_count <- gsub("[[:punct:][:cntrl:]]", "", info_count)
info_count <- gsub("건", "", info_count)
info_count
page <- cbind(tech_name, info_count)
page
info_count <- remove(subset(info_count,"건"))
info_count
# 건수
nodes <- html_nodes(text, "span.count")
info_count <- html_text(nodes, trim=TRUE)
info_count <- gsub("[[:punct:][:cntrl:]]", "", info_count)
info_count <- remove(subset(info_count,"건"))
info_count
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- c(1:length(tech_name)-1)
tech_name
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- c(tech_name[1:length(tech_name)-1])
tech_name
# 기술이름
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name <- c(tech_name[1:length(tech_name)-1])
tech_name
# 건수
nodes <- html_nodes(text, "span.count")
info_count <- html_text(nodes, trim=TRUE)
info_count <- gsub("[[:punct:][:cntrl:]]", "", info_count)
info_count <- c(info_count[2:length(info_count)])
info_count
page <- cbind(tech_name, info_count)
page
# 단일 페이지(rvest 패키지 사용)
install.packages("rvest");
library(rvest)
url<- "http://www.saramin.co.kr/zf_user/search?search_area=main&search_done=y&search_optional_item=n&searchType=default_mysearch&searchword=%EC%9E%90%EB%B0%94"
install.packages("rvest")
text <- read_html(url,  encoding="UTF-8")
install.packages("rvest")
library(rvest)
url<- "http://www.saramin.co.kr/zf_user/search?search_area=main&search_done=y&search_optional_item=n&searchType=default_mysearch&searchword=%EC%9E%90%EB%B0%94"
text <- read_html(url,  encoding="UTF-8")
# 기술이름
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- c(tech_name[1:length(tech_name)-1])
tech_name
# 건수
nodes <- html_nodes(text, "span.count")
info_count <- html_text(nodes, trim=TRUE)
info_count <- gsub("[[:punct:][:cntrl:]]", "", info_count)
info_count <- c(info_count[2:length(info_count)])
info_count
page <- cbind(tech_name, info_count)
write.csv(page, "saramin.csv")
info_count
# 건수
nodes <- html_nodes(text, "span.count")
info_count <- html_text(nodes, trim=TRUE)
info_count <- gsub("[[:punct:][:cntrl:]]", "", info_count)
info_count
subset(info_count,info_count!="건")
# 건수
nodes <- html_nodes(text, "span.count")
info_count <- html_text(nodes, trim=TRUE)
info_count <- gsub("[[:punct:][:cntrl:]]", "", info_count)
subset(info_count,info_count!="건")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name
# 기술이름
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- subset(tech_name,tech_name!="다른 필터항목 보기")
tech_name
library(rvest)
url<- "http://www.saramin.co.kr/zf_user/search?search_area=main&search_done=y&search_optional_item=n&searchType=default_mysearch&searchword=%EC%9E%90%EB%B0%94"
text <- read_html(url,  encoding="UTF-8")
# 기술이름
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- subset(tech_name,tech_name!="다른 필터항목 보기")
tech_name
# 건수
nodes <- html_nodes(text, "span.count")
info_count <- html_text(nodes, trim=TRUE)
info_count <- gsub("[[:punct:][:cntrl:]]", "", info_count)
info_count <- subset(info_count,info_count!="건")
info_count
page <- cbind(tech_name, info_count)
write.csv(page, "saramin.csv")
# 기술이름
nodes <- html_nodes(text, "label.lbl_sfilter span.txt")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name
# 기술이름
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name
tech_name
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- subset(tech_name,tech_name!="다른 필터항목 보기")
tech_name
nodes <- html_nodes(text, "span.txt")
tech_name <- html_text(nodes)
tech_name <- gsub("[[:punct:][:cntrl:]]", "", tech_name)
tech_name <- subset(tech_name,tech_name!="다른 필터항목 보기")
# 건수
nodes <- html_nodes(text, "span.count")
info_count <- html_text(nodes, trim=TRUE)
info_count <- gsub("[[:punct:][:cntrl:]]", "", info_count)
info_count <- subset(info_count,info_count!="건")
page <- cbind(tech_name, info_count)
write.csv(page, "saramin.csv",row.names = FALSE)
# 네이버 블로그 연동 : Rcurl 패키지 사용
install.packages("RCurl")
library(RCurl)
library(XML)
searchUrl<- "https://openapi.naver.com/v1/search/blog.xml"
Client_ID <- "izGsqP2exeThwwEUVU3x"
Client_Secret <- "WrwbQ1l6ZI"
query <- URLencode(iconv("여름추천요리","euc-kr","UTF-8"))
url<- paste(searchUrl, "?query=", query, "&display=20", sep="")
doc<- getURL(url, httpheader = c('Content-Type' = "application/xml",
'X-Naver-Client-Id' = Client_ID,'X-Naver-Client-Secret' = Client_Secret))
doc
URLencode("여름추천요리")
URLencode(iconv("여름추천요리","euc-kr","UTF-8"))
# 블로그 내용에 대한 리스트 만들기
doc2 <- htmlParse(doc, encoding="UTF-8")
text<- xpathSApply(doc2, "//item/description", xmlValue)
text
searchUrl<- "https://openapi.naver.com/v1/search/news.xml"
Client_ID <- "izGsqP2exeThwwEUVU3x"
Client_Secret <- "WrwbQ1l6ZI"
query <- URLencode(iconv("미세먼지","euc-kr","UTF-8"))
url<- paste(searchUrl, "?query=", query, "&display=20", sep="")
doc<- getURL(url, httpheader = c('Content-Type' = "application/xml",
'X-Naver-Client-Id' = Client_ID,'X-Naver-Client-Secret' = Client_Secret))
# 블로그 내용에 대한 리스트 만들기
doc2 <- htmlParse(doc, encoding="UTF-8")
text<- xpathSApply(doc2, "//item/description", xmlValue);
text
# 트위터 글 읽어오기
install.packages("twitteR")
library(twitteR)
api_key <- "gjUkHgO8bFmNobRk4g0Jas8xb"
api_secret <- "loF0mtnzLhtQDFjahdRHox6wcR1fiD6Fw95DP5QCSy3rLTTP1K"
access_token <- "607145164-8L5HtzopZzhjuBCgusUGKE3MHOa9P4RbmhUrM0E1"
access_token_secret <- "2wn2bsCA7JIH5DZ5Ss1deS5BNLabzaX2xSpM2ZLMIqwQf"
setup_twitter_oauth(api_key,api_secret, access_token,access_token_secret)
# oauth 정보 저장 확인
key <- "취업"
key <- enc2utf8(key)
result <- searchTwitter(key, n=100)
result
DF <- twListToDF(result)
DF
key <- enc2utf8(key)
result <- searchTwitter(key, n=100)
result <- searchTwitter(key, n=100)
result
DF <- twListToDF(result)
str(DF)
content <- DF$text
content
content <- gsub("[[:lower:][:upper:][:digit:][:punct:][:cntrl:]]", "", content)
content
